{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the full walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Update All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error : b'From https://github.com/CSSEGISandData/COVID-19\\n   37b2fa02..b6cf765a  master     -> origin/master\\n   4fe70bba..f759b906  web-data   -> origin/web-data\\n'\n",
      "out : b'Updating 37b2fa02..b6cf765a\\nFast-forward\\n csse_covid_19_data/README.md                       |    1 +\\n .../csse_covid_19_daily_reports/08-29-2020.csv     | 3951 ++++++++++++\\n .../csse_covid_19_daily_reports/08-30-2020.csv     | 3951 ++++++++++++\\n .../csse_covid_19_daily_reports_us/08-29-2020.csv  |   59 +\\n .../csse_covid_19_daily_reports_us/08-30-2020.csv  |   59 +\\n .../time_series_covid19_confirmed_US.csv           | 6682 ++++++++++----------\\n .../time_series_covid19_confirmed_global.csv       |  534 +-\\n .../time_series_covid19_deaths_US.csv              | 6682 ++++++++++----------\\n .../time_series_covid19_deaths_global.csv          |  534 +-\\n .../time_series_covid19_recovered_global.csv       |  508 +-\\n 10 files changed, 15491 insertions(+), 7470 deletions(-)\\n create mode 100644 csse_covid_19_data/csse_covid_19_daily_reports/08-29-2020.csv\\n create mode 100644 csse_covid_19_data/csse_covid_19_daily_reports/08-30-2020.csv\\n create mode 100644 csse_covid_19_data/csse_covid_19_daily_reports_us/08-29-2020.csv\\n create mode 100644 csse_covid_19_data/csse_covid_19_daily_reports_us/08-30-2020.csv\\n'\n"
     ]
    }
   ],
   "source": [
    "# %load ../src/data/get_data.py\n",
    "\"\"\"\n",
    "Created on Fri Aug 21 13:02:59 2020\n",
    "\n",
    "@author: Sriram\n",
    "\"\"\"\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Check Working directory and set the path\n",
    "if os.path.split(os.getcwd())[-1]=='notebooks':\n",
    "    os.chdir(\"../\")\n",
    "\n",
    "\n",
    "# Function to pull latest data from John Hopkins GITHUB page\n",
    "def get_john_hopkins():\n",
    "    'We use git pull to save the data in the folder COVID-19. Data saved as csv files under various names'\n",
    "    git_pull = subprocess.Popen( \"git pull\" , \n",
    "                     cwd = os.path.dirname( 'data/raw/COVID-19/' ), \n",
    "                     shell = True, \n",
    "                     stdout = subprocess.PIPE, \n",
    "                     stderr = subprocess.PIPE )\n",
    "    (out, error) = git_pull.communicate()\n",
    "    \n",
    "    print(\"Error : \" + str(error)) \n",
    "    print(\"out : \" + str(out))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    get_john_hopkins()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Process Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Sriram\\\\eds_covid-19'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking current working directory\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of rows stored: 59052\n",
      " Latest date is: 2020-08-30 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# %load src/data/process_JH_data\n",
    "\"\"\"\n",
    "Created on Fri Aug 21 18:59:53 2020\n",
    "\n",
    "@author: Sriram\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "#defining a function to process raw JH data into a relational data structure\n",
    "def store_relational_JH_data():\n",
    "    \"process raw JH data into a relational data structure\"\n",
    "    \n",
    "    data_path='data/raw/COVID-19/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'\n",
    "    pd_raw=pd.read_csv(data_path)\n",
    "    pd_data_base=pd_raw.rename(columns={'Country/Region':'country','Province/State':'state'})    \n",
    "    pd_data_base['state']=pd_data_base['state'].fillna('no')\n",
    "    pd_data_base=pd_data_base.drop(['Lat','Long'],axis=1)\n",
    "    pd_relational_model=pd_data_base.set_index(['state','country']) \\\n",
    "                                .T                              \\\n",
    "                                .stack(level=[0,1])             \\\n",
    "                                .reset_index()                  \\\n",
    "                                .rename(columns={'level_0':'date',\n",
    "                                                   0:'confirmed'},\n",
    "                                                      )\n",
    "    pd_relational_model['date']=pd_relational_model.date.astype('datetime64[ns]')\n",
    "    pd_relational_model.to_csv('data/processed/COVID_relational_confirmed.csv',sep=';',index=False)\n",
    "    print(' Number of rows stored: '+str(pd_relational_model.shape[0]))\n",
    "    print(' Latest date is: '+str(max(pd_relational_model.date)))\n",
    "#running the function\n",
    "if __name__ == '__main__':\n",
    "    store_relational_JH_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Filtering and Slope Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Sriram\\\\eds_covid-19'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking current working directory\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            date    state       country  confirmed  confirmed_filtered  \\\n",
      "0     2020-01-22  Alberta        Canada        0.0                 0.0   \n",
      "37518 2020-01-22       no  Korea, South        1.0                 0.8   \n",
      "37740 2020-01-22       no        Kosovo        0.0                 0.0   \n",
      "37962 2020-01-22       no        Kuwait        0.0                 0.0   \n",
      "38184 2020-01-22       no    Kyrgyzstan        0.0                 0.0   \n",
      "\n",
      "       confirmed_DR  confirmed_filtered_DR  \n",
      "0               NaN                    NaN  \n",
      "37518           NaN                    NaN  \n",
      "37740           NaN                    NaN  \n",
      "37962           NaN                    NaN  \n",
      "38184           NaN                    NaN  \n"
     ]
    }
   ],
   "source": [
    "# %load src/features/build_features.py\n",
    "\"\"\"\n",
    "Created on Sat Aug 22 10:32:53 2020\n",
    "\n",
    "@author: Sriram\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "\n",
    "# we define the linear regression object\n",
    "reg=linear_model.LinearRegression(fit_intercept=True)\n",
    "\n",
    "def get_doubling_time_via_regression(in_array):\n",
    "    \" Use linear regression to find the doubling rate\"\n",
    "    y=np.array(in_array)\n",
    "    X=np.arange(-1,2).reshape(-1,1)\n",
    "    # for safety we are asserting that the length of the input array is 3\n",
    "    assert len(in_array)==3\n",
    "    reg.fit(X,y)\n",
    "    intercept=reg.intercept_\n",
    "    slope=reg.coef_\n",
    "    return intercept/slope\n",
    "\n",
    "def savgol_filter(df_input,column='confirmed',window=5):\n",
    "    df_result=df_input\n",
    "    degree=1\n",
    "    # we fill the missing entries with zero\n",
    "    filter_in=df_input[column].fillna(0)\n",
    "    result=signal.savgol_filter(np.array(filter_in),\n",
    "                        window,\n",
    "                        degree)\n",
    "    df_result[str(column+'_filtered')]=result\n",
    "    return df_result\n",
    "\n",
    "def rolling_reg(df_input,col='confirmed'):\n",
    "    \"Input is dataframe\"\n",
    "    \"return value is a single series of doubling rates\"\n",
    "    days_back=3\n",
    "    result=df_input[col].rolling(window=days_back,min_periods=days_back).apply(get_doubling_time_via_regression,raw=False)\n",
    "    return result\n",
    "\n",
    "def calc_filtered_data(df_input,filter_on='confirmed'):\n",
    "    \"Apply SavGol filter on the dataset and return the merged dataset\"\n",
    "    must_contain=set(['state','country',filter_on])\n",
    "    assert must_contain.issubset(set(df_input.columns)),'Error in calc_filtered_data not all columns in data Frame'\n",
    "    df_output=df_input.copy()\n",
    "    pd_filtered_result=df_output[['state','country',filter_on]].groupby(['state','country']).apply(savgol_filter)#.reset_index()\n",
    "    df_output=pd.merge(df_output,pd_filtered_result[[str(filter_on+'_filtered')]],left_index=True,right_index=True,how='left')\n",
    "    \n",
    "    return df_output.copy()\n",
    "\n",
    "\n",
    "def calc_doubling_rate(df_input,filter_on='confirmed'):\n",
    "    \"Calculate doubling rate and return the dataframe\"\n",
    "    must_contain=set(['state','country',filter_on])\n",
    "    assert must_contain.issubset(set(df_input.columns)),'Error in calc_filtered_data not all columns in data Frame'\n",
    "    pd_DR_result=df_input[['state','country',filter_on]].groupby(['state','country']).apply(rolling_reg,filter_on).reset_index()\n",
    "    pd_DR_result=pd_DR_result.rename(columns={filter_on:filter_on+'_DR','level_2':'index'})\n",
    "    df_output=pd.merge(df_input,pd_DR_result[['index',str(filter_on+'_DR')]],left_index=True,right_on=['index'],how='left')\n",
    "    df_output=df_output.drop(columns=['index'])\n",
    "    return df_output\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    #test_data=np.array([2,4,6])\n",
    "    #doubling_time=get_doubling_time_via_regression(test_data)\n",
    "    #print('Test slope is :'+str(doubling_time))\n",
    "    # We read the data from file\n",
    "    pd_JH_data=pd.read_csv('data/processed/COVID_relational_confirmed.csv',sep=';',parse_dates=[0])\n",
    "    pd_JH_data=pd_JH_data.sort_values('date',ascending=True).reset_index(drop=True).copy()\n",
    "    # We process the data calculating filtered data and doubling rate\n",
    "    pd_JH_result_large=calc_filtered_data(pd_JH_data)\n",
    "    pd_JH_result_large=calc_doubling_rate(pd_JH_result_large)\n",
    "    pd_JH_result_large=calc_doubling_rate(pd_JH_result_large,filter_on='confirmed_filtered')\n",
    "    # we apply a threshold on confirmed column since if values are small doubling rate goes to infinity\n",
    "    mask=pd_JH_result_large['confirmed']>100\n",
    "    pd_JH_result_large['confirmed_filtered_DR']=pd_JH_result_large['confirmed_filtered_DR'].where(mask,other=np.NaN)\n",
    "    pd_JH_result_large.to_csv('data/processed/COVID_final_set.csv',sep=';',index=False)\n",
    "    print(pd_JH_result_large.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Dynamic DashBoard for COVID-19 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Sriram\\\\eds_covid-19'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on http://127.0.0.1:8050/\n",
      "Running on http://127.0.0.1:8050/\n",
      "Running on http://127.0.0.1:8050/\n",
      "Running on http://127.0.0.1:8050/\n",
      "Running on http://127.0.0.1:8050/\n",
      "Running on http://127.0.0.1:8050/\n",
      "Running on http://127.0.0.1:8050/\n",
      "Running on http://127.0.0.1:8050/\n",
      "Running on http://127.0.0.1:8050/\n",
      "Running on http://127.0.0.1:8050/\n",
      "Running on http://127.0.0.1:8050/\n",
      "Running on http://127.0.0.1:8050/\n",
      "Running on http://127.0.0.1:8050/\n",
      "Running on http://127.0.0.1:8050/\n",
      "Running on http://127.0.0.1:8050/\n",
      "Running on http://127.0.0.1:8050/\n",
      "Running on http://127.0.0.1:8050/\n",
      "Running on http://127.0.0.1:8050/\n",
      "Running on http://127.0.0.1:8050/\n",
      "Running on http://127.0.0.1:8050/\n",
      "Running on http://127.0.0.1:8050/\n",
      "Running on http://127.0.0.1:8050/\n",
      "Running on http://127.0.0.1:8050/\n",
      "Running on http://127.0.0.1:8050/\n",
      "Running on http://127.0.0.1:8050/\n",
      "Running on http://127.0.0.1:8050/\n",
      "Running on http://127.0.0.1:8050/\n",
      "Running on http://127.0.0.1:8050/\n",
      "Running on http://127.0.0.1:8050/\n",
      "Debugger PIN: 620-455-361\n",
      "Debugger PIN: 620-455-361\n",
      "Debugger PIN: 620-455-361\n",
      "Debugger PIN: 620-455-361\n",
      "Debugger PIN: 620-455-361\n",
      "Debugger PIN: 620-455-361\n",
      "Debugger PIN: 620-455-361\n",
      "Debugger PIN: 620-455-361\n",
      "Debugger PIN: 620-455-361\n",
      "Debugger PIN: 620-455-361\n",
      "Debugger PIN: 620-455-361\n",
      "Debugger PIN: 620-455-361\n",
      "Debugger PIN: 620-455-361\n",
      "Debugger PIN: 620-455-361\n",
      "Debugger PIN: 620-455-361\n",
      "Debugger PIN: 620-455-361\n",
      "Debugger PIN: 620-455-361\n",
      "Debugger PIN: 620-455-361\n",
      "Debugger PIN: 620-455-361\n",
      "Debugger PIN: 620-455-361\n",
      "Debugger PIN: 620-455-361\n",
      "Debugger PIN: 620-455-361\n",
      "Debugger PIN: 620-455-361\n",
      "Debugger PIN: 620-455-361\n",
      "Debugger PIN: 620-455-361\n",
      "Debugger PIN: 620-455-361\n",
      "Debugger PIN: 620-455-361\n",
      "Debugger PIN: 620-455-361\n",
      "Debugger PIN: 620-455-361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Tip: There are .env or .flaskenv files present. Do \"pip install python-dotenv\" to use them.\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: on\n"
     ]
    }
   ],
   "source": [
    "# %load src/visualization/visualize.py\n",
    "\"\"\"\n",
    "Created on Mon Aug 24 17:00:40 2020\n",
    "\n",
    "@author: Sriram\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input,Output\n",
    "\n",
    "df_input_large=pd.read_csv('data/processed/COVID_final_set.csv',sep=';')\n",
    "\n",
    "fig=go.Figure()\n",
    "app=dash.Dash()\n",
    "app.layout=html.Div([\n",
    "        dcc.Markdown('''\n",
    "                     # Applied Datascience on COVID-19 Data\n",
    "                     Goal of the project is to create a responsive Dashboard \n",
    "                     with data from many countries in an automated way through:\n",
    "                    data gathering , data transformation,\n",
    "                    filtering and machine learning to approximate doubling time.\n",
    "                    '''),\n",
    "        # For Country dropdown menu\n",
    "        dcc.Markdown(''' ## Multi-Select Country for Visualization'''),\n",
    "        \n",
    "        dcc.Dropdown( id='country_drop_down',\n",
    "                     options=[{'label':each,'value':each} for each in df_input_large['country'].unique()],\n",
    "                     value=['Germany','India','US'],\n",
    "                     multi=True),\n",
    "        # For Doubling rate or conformed cased drop down mneu\n",
    "        dcc.Markdown(''' ## Select Timeline of confirmed COVID-19 cases or approximated doubling time'''),\n",
    "        \n",
    "        dcc.Dropdown( id='doubling_time',\n",
    "                     options=[\n",
    "                             {'label':'Timeline Confirmed','value':'confirmed'},\n",
    "                             {'label':'Timeline Confirmed Filtered','value':'confirmed_filtered'},\n",
    "                             {'label':'Timeline Doubling Rate','value':'confirmed_DR'},\n",
    "                             {'label':'Timeline Doubling Rate Filtered','value':'confirmed_filtered_DR'},\n",
    "                             ],\n",
    "                     value='confirmed',\n",
    "                     multi=False),\n",
    "        dcc.Graph(figure=fig,id='main_window_slope')\n",
    "        \n",
    "                    ])\n",
    "\n",
    "@app.callback(\n",
    "    Output('main_window_slope', 'figure'),\n",
    "    [Input('country_drop_down', 'value'),\n",
    "    Input('doubling_time', 'value')])\n",
    "def update_figure(country_list,show_doubling):\n",
    "    if 'DR' in show_doubling:\n",
    "        my_yaxis={'type':\"log\",\n",
    "               'title':'Approximated doubling rate over 3 days'\n",
    "                 }\n",
    "    else:\n",
    "        my_yaxis={'type':\"log\",\n",
    "                  'title':'Confirmed infected people (source: johns hopkins csse, log-scale)'\n",
    "                 }\n",
    "    #Define the traces for the countries\n",
    "    traces = []\n",
    "    for each in country_list:\n",
    "        df_plot=df_input_large[df_input_large['country']==each]\n",
    "        if show_doubling=='confirmed_filtered_DR':\n",
    "            df_plot=df_plot[['state','country','confirmed','confirmed_filtered','confirmed_DR','confirmed_filtered_DR','date']].groupby(['country','date']).agg(np.mean).reset_index()\n",
    "        else:\n",
    "            df_plot=df_plot[['state','country','confirmed','confirmed_filtered','confirmed_DR','confirmed_filtered_DR','date']].groupby(['country','date']).agg(np.sum).reset_index()\n",
    "       \n",
    "        traces.append(dict(x=df_plot.date,\n",
    "                                y=df_plot[show_doubling],\n",
    "                                mode='markers+lines',\n",
    "                                opacity=0.9,\n",
    "                                name=each\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "    return {'data':traces,\n",
    "            'layout':dict(\n",
    "                width=1280,\n",
    "                height=600,\n",
    "                xaxis={'title':'Timeline',\n",
    "                        'tickangle':-45,\n",
    "                        'nticks':20,\n",
    "                        'tickfont':dict(size=14,color=\"#7f7f7f\")},\n",
    "                yaxis=my_yaxis)\n",
    "            }\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    app.run_server(debug=True, use_reloader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
